{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Math Project Notebook\n",
    "\n",
    "This is a notebook for supervised machine learning project in Nueral Network Mathematics class. \n",
    "\n",
    "Group members: Luke, Akshay, Yile\n",
    "\n",
    "#### variable names explanation:\n",
    "| Var name | Feature name | Description|\n",
    "|---|---|---|\n",
    "|pos      | Num posts    | Number of total posts that the user has ever posted.|\n",
    "|flg      | Num following | Number of following|\n",
    "|flr      | Num followers | Number of followers|\n",
    "|bl | Biography length | Length (number of characters) of the user's biography|\n",
    "|pic | Picture availability | Value 0 if the user has no profile picture, or 1 if has|\n",
    "|lin | Link availability | Value 0 if the user has no external URL, or 1 if has|\n",
    "|cl | Average caption length | The average number of character of captions in media|\n",
    "|cz | Caption zero | Percentage (0.0 to 1.0) of captions that has almost zero (<=3) length|\n",
    "|ni | Non image percentage | Percentage (0.0 to 1.0) of non-image media. There are three types of media on an Instagram post, i.e. image, video, carousel|\n",
    "|erl | Engagement rate (Like) | Engagement rate (ER) is commonly defined as (num likes) divide by (num media) divide by (num followers)|\n",
    "|erc | Engagement rate (Comm.) | Similar to ER like, but it is for comments|\n",
    "|lt | Location tag percentage | Percentage (0.0 to 1.0) of posts tagged with location|\n",
    "|hc | Average hashtag count | Average number of hashtags used in a post|\n",
    "|pr | Promotional keywords | Average use of promotional keywords in hashtag, i.e. {regrann, contest, repost, giveaway, mention, share, give away, quiz}|\n",
    "|fo | Followers keywords | Average use of followers hunter keywords in hashtag, i.e. {follow, like, folback, follback, f4f}|\n",
    "|cs | Cosine similarity | Average cosine similarity of between all pair of two posts a user has|\n",
    "|pi | Post interval | Average interval between posts (in hours)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "df_data = pd.read_csv(\"data/user_fake_authentic_2class.csv\")\n",
    "# training features size: 65326 x 17\n",
    "data_x = df_data.iloc[:,:-1]\n",
    "\n",
    "# label types: r=real and f=fake\n",
    "data_y = df_data.iloc[:,-1:]\n",
    "# convert to 0:fake, 1:real\n",
    "data_y = data_y.replace({'class':{\"r\": 1, \"f\":0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize \n",
    "norm_x = preprocessing.normalize(data_x)\n",
    "norm_x = pd.DataFrame(norm_x, columns=data_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic probability model is\n",
    "\n",
    "$ \\hat{p}(s, \\theta) = [1 + \\exp{-\\hat{y}(s, \\theta)}]^{-1} $\n",
    "\n",
    "The $\\hat{y}$ is defined as:\n",
    "\n",
    "$ \\hat{y}(s, \\theta) = \\theta^T [s^T 1]^T  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic probability model\n",
    "def y_hat(si, theta, y = np.shape(data_y)[0]):\n",
    "        return theta.T * (si.T * np.ones(len(y))).T\n",
    "        \n",
    "def p_logistic(si, theta):\n",
    "    return (1 + np.exp(-y_hat(si, theta)))^(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function is defined as\n",
    "\n",
    "$ c([y,s], \\theta) = - y  log\\hat{p}(s, \\theta) - (1-y)log(1-\\hat{p}(s, \\theta)) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective function\n",
    "def obj(yi, y_hat):\n",
    "    return -yi * np.log(y_hat) - (1-yi)*np.log(1-y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is\n",
    "\n",
    "$ l_{n}(\\theta) = -(1/n)\\sum_{i=1}^{n} c([y,s], \\theta) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "start = time.time()\n",
    "def loss(theta, yi, si, n):\n",
    "    return - (1/n) * sum(obj(yi, si, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient equation is\n",
    "\n",
    "$ \\frac{dc_{i}}{d\\theta} = -(y_i - \\hat{y}_i) [s_i^{T}, 1] $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient\n",
    "\n",
    "def gradient_descent(theta, yi, si, n, max_iter = 200, gamma = 0.1):\n",
    "    grad = []\n",
    "    t = 0\n",
    "    gradnorm = np.inf\n",
    "    while gradnorm >= 10e-4 & t < max_iter:\n",
    "        gt = loss(theta, yi, si, n)\n",
    "        theta = theta - gamma*gt\n",
    "        gradnorm = max(abs(gt))\n",
    "        print(f\"Iternation: {t}; gradnorm = {gradnorm}\")\n",
    "        grad.append(gradnorm)\n",
    "    return theta, grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tvbenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3c26eedd07840027ff202a94d88c89e67a86d8b5dcd58f087e1d46a589dbbcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
