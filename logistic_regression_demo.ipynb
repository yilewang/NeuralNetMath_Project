{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Math Project Notebook\n",
    "\n",
    "This is a notebook for supervised machine learning project in Neural Network Mathematics class. \n",
    "\n",
    "Group members: Luke, Akshay, Yile\n",
    "\n",
    "#### variable names explanation:\n",
    "| Var name | Feature name | Description|\n",
    "|---|---|---|\n",
    "|pos      | Num posts    | Number of total posts that the user has ever posted.|\n",
    "|flg      | Num following | Number of following|\n",
    "|flr      | Num followers | Number of followers|\n",
    "|bl | Biography length | Length (number of characters) of the user's biography|\n",
    "|pic | Picture availability | Value 0 if the user has no profile picture, or 1 if has|\n",
    "|lin | Link availability | Value 0 if the user has no external URL, or 1 if has|\n",
    "|cl | Average caption length | The average number of character of captions in media|\n",
    "|cz | Caption zero | Percentage (0.0 to 1.0) of captions that has almost zero (<=3) length|\n",
    "|ni | Non image percentage | Percentage (0.0 to 1.0) of non-image media. There are three types of media on an Instagram post, i.e. image, video, carousel|\n",
    "|erl | Engagement rate (Like) | Engagement rate (ER) is commonly defined as (num likes) divide by (num media) divide by (num followers)|\n",
    "|erc | Engagement rate (Comm.) | Similar to ER like, but it is for comments|\n",
    "|lt | Location tag percentage | Percentage (0.0 to 1.0) of posts tagged with location|\n",
    "|hc | Average hashtag count | Average number of hashtags used in a post|\n",
    "|pr | Promotional keywords | Average use of promotional keywords in hashtag, i.e. {regrann, contest, repost, giveaway, mention, share, give away, quiz}|\n",
    "|fo | Followers keywords | Average use of followers hunter keywords in hashtag, i.e. {follow, like, folback, follback, f4f}|\n",
    "|cs | Cosine similarity | Average cosine similarity of between all pair of two posts a user has|\n",
    "|pi | Post interval | Average interval between posts (in hours)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic probability model is\n",
    "\n",
    "$ \\hat{p}(s, \\theta) = [1 + e^{-\\hat{y}(s, \\theta)}]^{-1} $\n",
    "\n",
    "The $\\hat{y}$ is defined as:\n",
    "\n",
    "$ \\hat{y}(s, \\theta) = \\theta^T [s^T 1]^T  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function is defined as\n",
    "\n",
    "$ c([y,s], \\theta) = - y  log\\hat{p}(s, \\theta) - (1-y)log(1-\\hat{p}(s, \\theta)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is\n",
    "\n",
    "$ l_{n}(\\theta) = -(1/n)\\sum_{i=1}^{n} c([y,s], \\theta) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent process is\n",
    "\n",
    "$ \\frac{dc_{i}}{d\\theta} = -(y_i - \\hat{y}_i) [s_i^{T}, 1] $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>flw</th>\n",
       "      <th>flg</th>\n",
       "      <th>bl</th>\n",
       "      <th>pic</th>\n",
       "      <th>lin</th>\n",
       "      <th>cl</th>\n",
       "      <th>cz</th>\n",
       "      <th>ni</th>\n",
       "      <th>erl</th>\n",
       "      <th>erc</th>\n",
       "      <th>lt</th>\n",
       "      <th>hc</th>\n",
       "      <th>pr</th>\n",
       "      <th>fo</th>\n",
       "      <th>cs</th>\n",
       "      <th>pi</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>48</td>\n",
       "      <td>325</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.094985</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>66</td>\n",
       "      <td>321</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>14.390000</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.206826</td>\n",
       "      <td>230.412857</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>970</td>\n",
       "      <td>308</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.572174</td>\n",
       "      <td>43.569939</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>360</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.859799</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>285</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.290000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300494</td>\n",
       "      <td>0.126019</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65321</th>\n",
       "      <td>13</td>\n",
       "      <td>145</td>\n",
       "      <td>642</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.270000</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>1745.291260</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65322</th>\n",
       "      <td>652</td>\n",
       "      <td>3000</td>\n",
       "      <td>1300</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389</td>\n",
       "      <td>8.520000</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.169917</td>\n",
       "      <td>54.629120</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65323</th>\n",
       "      <td>1500</td>\n",
       "      <td>3700</td>\n",
       "      <td>3200</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111</td>\n",
       "      <td>9.390000</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.058908</td>\n",
       "      <td>129.802048</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65324</th>\n",
       "      <td>329</td>\n",
       "      <td>1500</td>\n",
       "      <td>1800</td>\n",
       "      <td>218</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>290</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.103174</td>\n",
       "      <td>53.402840</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65325</th>\n",
       "      <td>206</td>\n",
       "      <td>659</td>\n",
       "      <td>608</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333</td>\n",
       "      <td>25.549999</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.017505</td>\n",
       "      <td>604.981445</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65326 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pos   flw   flg   bl  pic  lin   cl        cz     ni        erl   erc  \\\n",
       "0        44    48   325   33    1    0   12  0.000000  0.000   0.000000  0.00   \n",
       "1        10    66   321  150    1    0  213  0.000000  1.000  14.390000  1.97   \n",
       "2        33   970   308  101    1    1  436  0.000000  1.000  10.100000  0.30   \n",
       "3        70    86   360   14    1    0    0  1.000000  0.000   0.780000  0.06   \n",
       "4         3    21   285   73    1    0   93  0.000000  0.000  14.290000  0.00   \n",
       "...     ...   ...   ...  ...  ...  ...  ...       ...    ...        ...   ...   \n",
       "65321    13   145   642    0    1    0    7  0.461538  0.000  14.270000  0.58   \n",
       "65322   652  3000  1300  146    1    1  384  0.000000  0.389   8.520000  0.13   \n",
       "65323  1500  3700  3200  147    1    1  129  0.000000  0.111   9.390000  0.31   \n",
       "65324   329  1500  1800  218    1    1  290  0.055556  0.000   6.350000  0.26   \n",
       "65325   206   659   608   27    1    0   77  0.000000  0.333  25.549999  0.53   \n",
       "\n",
       "          lt     hc   pr     fo        cs           pi class  \n",
       "0      0.000  0.000  0.0  0.000  0.111111     0.094985     f  \n",
       "1      0.000  1.500  0.0  0.000  0.206826   230.412857     f  \n",
       "2      0.000  2.500  0.0  0.056  0.572174    43.569939     f  \n",
       "3      0.000  0.000  0.0  0.000  1.000000     5.859799     f  \n",
       "4      0.667  0.000  0.0  0.000  0.300494     0.126019     f  \n",
       "...      ...    ...  ...    ...       ...          ...   ...  \n",
       "65321  0.000  0.077  0.0  0.000  0.192308  1745.291260     r  \n",
       "65322  0.000  1.611  0.0  0.000  0.169917    54.629120     r  \n",
       "65323  0.722  0.000  0.0  0.056  0.058908   129.802048     r  \n",
       "65324  0.222  0.500  0.0  0.000  0.103174    53.402840     r  \n",
       "65325  0.222  0.222  0.0  0.167  0.017505   604.981445     r  \n",
       "\n",
       "[65326 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "df_data = pd.read_csv(\"data/user_fake_authentic_2class.csv\")\n",
    "# training features size: 65326 x 17\n",
    "data_x = df_data.iloc[:,:-1]\n",
    "\n",
    "# label types: r=real and f=fake\n",
    "data_y = df_data.iloc[:,-1:]\n",
    "# convert to 0:fake, 1:real\n",
    "data_y = data_y.replace({'class':{\"r\": 1, \"f\":0}})\n",
    "\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>flw</th>\n",
       "      <th>flg</th>\n",
       "      <th>bl</th>\n",
       "      <th>pic</th>\n",
       "      <th>lin</th>\n",
       "      <th>cl</th>\n",
       "      <th>cz</th>\n",
       "      <th>ni</th>\n",
       "      <th>erl</th>\n",
       "      <th>erc</th>\n",
       "      <th>lt</th>\n",
       "      <th>hc</th>\n",
       "      <th>pr</th>\n",
       "      <th>fo</th>\n",
       "      <th>cs</th>\n",
       "      <th>pi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.132007</td>\n",
       "      <td>0.144008</td>\n",
       "      <td>0.975053</td>\n",
       "      <td>0.099005</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020912</td>\n",
       "      <td>0.138019</td>\n",
       "      <td>0.671273</td>\n",
       "      <td>0.313679</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.030092</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.481838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029645</td>\n",
       "      <td>0.871381</td>\n",
       "      <td>0.276686</td>\n",
       "      <td>0.090731</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.391672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.009073</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.039140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.185676</td>\n",
       "      <td>0.228116</td>\n",
       "      <td>0.954904</td>\n",
       "      <td>0.037135</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.015543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009690</td>\n",
       "      <td>0.067827</td>\n",
       "      <td>0.920511</td>\n",
       "      <td>0.235780</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65321</th>\n",
       "      <td>0.006969</td>\n",
       "      <td>0.077732</td>\n",
       "      <td>0.344165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.935621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65322</th>\n",
       "      <td>0.194071</td>\n",
       "      <td>0.892962</td>\n",
       "      <td>0.386950</td>\n",
       "      <td>0.043458</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.114299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.016261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65323</th>\n",
       "      <td>0.292853</td>\n",
       "      <td>0.722370</td>\n",
       "      <td>0.624752</td>\n",
       "      <td>0.028700</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.025185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.025342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65324</th>\n",
       "      <td>0.137409</td>\n",
       "      <td>0.626483</td>\n",
       "      <td>0.751780</td>\n",
       "      <td>0.091049</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.121120</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.022304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65325</th>\n",
       "      <td>0.186527</td>\n",
       "      <td>0.596705</td>\n",
       "      <td>0.550526</td>\n",
       "      <td>0.024448</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.547793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65326 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pos       flw       flg        bl       pic       lin        cl  \\\n",
       "0      0.132007  0.144008  0.975053  0.099005  0.003000  0.000000  0.036002   \n",
       "1      0.020912  0.138019  0.671273  0.313679  0.002091  0.000000  0.445424   \n",
       "2      0.029645  0.871381  0.276686  0.090731  0.000898  0.000898  0.391672   \n",
       "3      0.185676  0.228116  0.954904  0.037135  0.002653  0.000000  0.000000   \n",
       "4      0.009690  0.067827  0.920511  0.235780  0.003230  0.000000  0.300377   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "65321  0.006969  0.077732  0.344165  0.000000  0.000536  0.000000  0.003753   \n",
       "65322  0.194071  0.892962  0.386950  0.043458  0.000298  0.000298  0.114299   \n",
       "65323  0.292853  0.722370  0.624752  0.028700  0.000195  0.000195  0.025185   \n",
       "65324  0.137409  0.626483  0.751780  0.091049  0.000418  0.000418  0.121120   \n",
       "65325  0.186527  0.596705  0.550526  0.024448  0.000905  0.000000  0.069721   \n",
       "\n",
       "             cz        ni       erl       erc        lt        hc   pr  \\\n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "1      0.000000  0.002091  0.030092  0.004120  0.000000  0.003137  0.0   \n",
       "2      0.000000  0.000898  0.009073  0.000269  0.000000  0.002246  0.0   \n",
       "3      0.002653  0.000000  0.002069  0.000159  0.000000  0.000000  0.0   \n",
       "4      0.000000  0.000000  0.046155  0.000000  0.002154  0.000000  0.0   \n",
       "...         ...       ...       ...       ...       ...       ...  ...   \n",
       "65321  0.000247  0.000000  0.007650  0.000311  0.000000  0.000041  0.0   \n",
       "65322  0.000000  0.000116  0.002536  0.000039  0.000000  0.000480  0.0   \n",
       "65323  0.000000  0.000022  0.001833  0.000061  0.000141  0.000000  0.0   \n",
       "65324  0.000023  0.000000  0.002652  0.000109  0.000093  0.000209  0.0   \n",
       "65325  0.000000  0.000302  0.023135  0.000480  0.000201  0.000201  0.0   \n",
       "\n",
       "             fo        cs        pi  \n",
       "0      0.000000  0.000333  0.000285  \n",
       "1      0.000000  0.000433  0.481838  \n",
       "2      0.000050  0.000514  0.039140  \n",
       "3      0.000000  0.002653  0.015543  \n",
       "4      0.000000  0.000971  0.000407  \n",
       "...         ...       ...       ...  \n",
       "65321  0.000000  0.000103  0.935621  \n",
       "65322  0.000000  0.000051  0.016261  \n",
       "65323  0.000011  0.000012  0.025342  \n",
       "65324  0.000000  0.000043  0.022304  \n",
       "65325  0.000151  0.000016  0.547793  \n",
       "\n",
       "[65326 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize \n",
    "norm_x = preprocessing.normalize(data_x)\n",
    "norm_x = pd.DataFrame(norm_x, columns=data_x.columns)\n",
    "\n",
    "norm_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd? Could implement a PCA for dimension reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class for logistic regression\n",
    "\n",
    "\"\"\"\n",
    "In our logistic regression model, there are several parameters need to be pre-defined:\n",
    "    1. gamma, learning rate\n",
    "    2. max_iters, the iteration number for the gradient descent\n",
    "    3. data_x, the training dataset\n",
    "    4. data_y, the prediction outcome\n",
    "\"\"\"\n",
    "\n",
    "class Modeling:\n",
    "\n",
    "    def __init__(self, theta, gamma = 0.0001, max_iters = 1000):\n",
    "        self.gamma = gamma\n",
    "        self.max_iters = max_iters\n",
    "        self.theta = theta\n",
    "        self.grad = None\n",
    "    \n",
    "\n",
    "    def _S_one(self, data_x):\n",
    "        data_x_yhat = data_x\n",
    "        data_x_yhat[\"y_hat\"] = np.ones(len(data_x.index))\n",
    "        return data_x_yhat\n",
    "    \n",
    "            \n",
    "    def _cost_function(self, theta, S, data_y):\n",
    "        y_pred = self._logistic(np.dot(S, theta))\n",
    "        cost = np.log(y_pred)*data_y + (1-data_y)* np.log(1-y_pred)\n",
    "        loss = 1/len(data_y) * sum(cost)\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def _gradient_descent_func(self, data_y, y_hat, data_x_yhat_t):\n",
    "        gradient = y_hat*(1 - y_hat) @ np.array(data_x_yhat_t).T\n",
    "        #gradient = np.matmul(-1*(np.array(data_y).flatten() - y_hat).T, np.array(data_x_yhat))\n",
    "        gradient_vector = gradient.flatten()\n",
    "        return gradient_vector\n",
    "\n",
    "    # the objective function, y_hat = the prediction results from logistic regression\n",
    "    def gradient_iteration(self, data_x, data_y):\n",
    "        # gradient descent\n",
    "        grad = []\n",
    "        t = 0\n",
    "        gradnorm = np.inf\n",
    "        while gradnorm >= 0.001 and t <= self.max_iters:\n",
    "            data_x_yhat_t  = self._s_t_one(data_x)\n",
    "            y_hat = self._y_hat_func(data_x_yhat_t)\n",
    "            print(np.shape(y_hat))\n",
    "            # The gradient descent \n",
    "            gradient_loss = self._gradient_descent_func(data_y, y_hat, data_x_yhat_t)\n",
    "            gt = gradient_loss\n",
    "            print(np.shape(gt))\n",
    "            self.theta = self.theta - self.gamma*gt\n",
    "            gradnorm = max(abs(gt))\n",
    "            t += 1\n",
    "            # print(f\"Iternation: {t}; gradnorm = {gradnorm}\")\n",
    "            grad.append(gradnorm)\n",
    "        return self.theta, grad\n",
    "        \n",
    "    def _logistic(self, x):\n",
    "        return (1/(1+np.exp(-x)))\n",
    "\n",
    "    def fitting(self, data_x):\n",
    "        data_x_y_hat = self._s_one(data_x)\n",
    "        y_hat_prob = self._logistic(self._y_hat_func(data_x_y_hat))\n",
    "        y_hat_binary = [1 if i>0.5 else 0 for i in y_hat_prob]\n",
    "        return y_hat_binary, y_hat_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (18,) and (19, 65326) not aligned",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tvbenv/lib/python3.9/site-packages/pandas/core/frame.py:1560\u001b[0m, in \u001b[0;36mDataFrame.__rmatmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1560\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mT\u001b[39m.\u001b[39;49mdot(np\u001b[39m.\u001b[39;49mtranspose(other))\u001b[39m.\u001b[39mT\n\u001b[1;32m   1561\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/tvbenv/lib/python3.9/site-packages/pandas/core/frame.py:1518\u001b[0m, in \u001b[0;36mDataFrame.dot\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[39mif\u001b[39;00m lvals\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m rvals\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m-> 1518\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1519\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDot product shape mismatch, \u001b[39m\u001b[39m{\u001b[39;00mlvals\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m{\u001b[39;00mrvals\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1520\u001b[0m         )\n\u001b[1;32m   1522\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, DataFrame):\n",
      "\u001b[0;31mValueError\u001b[0m: Dot product shape mismatch, (65326, 19) vs (18,)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# find the values\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m Modeling(theta \u001b[39m=\u001b[39m init_theta, gamma \u001b[39m=\u001b[39m \u001b[39m0.00001\u001b[39m, max_iters\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m thetas, grad \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgradient_iteration(norm_x, data_y)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# y_hat_binary, y_hat_prob = model.fitting(norm_x)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;32m/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb Cell 11\u001b[0m in \u001b[0;36mModeling.gradient_iteration\u001b[0;34m(self, data_x, data_y)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mwhile\u001b[39;00m gradnorm \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m \u001b[39mand\u001b[39;00m t \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iters:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     data_x_yhat_t  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_s_t_one(data_x)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_y_hat_func(data_x_yhat_t)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39mshape(y_hat))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39m# The gradient descent \u001b[39;00m\n",
      "\u001b[1;32m/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb Cell 11\u001b[0m in \u001b[0;36mModeling._y_hat_func\u001b[0;34m(self, data_x_yhat)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_y_hat_func\u001b[39m(\u001b[39mself\u001b[39m, data_x_yhat):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     y_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtheta)\u001b[39m.\u001b[39;49mT \u001b[39m@\u001b[39;49m data_x_yhat\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m y_hat\n",
      "File \u001b[0;32m~/miniconda3/envs/tvbenv/lib/python3.9/site-packages/pandas/core/generic.py:2101\u001b[0m, in \u001b[0;36mNDFrame.__array_ufunc__\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m   2098\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array_ufunc__\u001b[39m(\n\u001b[1;32m   2099\u001b[0m     \u001b[39mself\u001b[39m, ufunc: np\u001b[39m.\u001b[39mufunc, method: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39minputs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m   2100\u001b[0m ):\n\u001b[0;32m-> 2101\u001b[0m     \u001b[39mreturn\u001b[39;00m arraylike\u001b[39m.\u001b[39;49marray_ufunc(\u001b[39mself\u001b[39;49m, ufunc, method, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tvbenv/lib/python3.9/site-packages/pandas/core/arraylike.py:263\u001b[0m, in \u001b[0;36marray_ufunc\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    262\u001b[0m \u001b[39m# for binary ops, use our custom dunder methods\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m result \u001b[39m=\u001b[39m maybe_dispatch_ufunc_to_dunder_op(\u001b[39mself\u001b[39;49m, ufunc, method, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/tvbenv/lib/python3.9/site-packages/pandas/_libs/ops_dispatch.pyx:113\u001b[0m, in \u001b[0;36mpandas._libs.ops_dispatch.maybe_dispatch_ufunc_to_dunder_op\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tvbenv/lib/python3.9/site-packages/pandas/core/frame.py:1566\u001b[0m, in \u001b[0;36mDataFrame.__rmatmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[39m# GH#21581 give exception message for original shapes\u001b[39;00m\n\u001b[1;32m   1565\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mshape(other)\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m not aligned\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1566\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (18,) and (19, 65326) not aligned"
     ]
    }
   ],
   "source": [
    "# model training process\n",
    "start = time.time()\n",
    "# initiate the theta\n",
    "init_theta = np.zeros(len(df_data.columns))\n",
    "# find the values\n",
    "model = Modeling(theta = init_theta, gamma = 0.00001, max_iters=1000)\n",
    "thetas, grad = model.gradient_iteration(norm_x, data_y)\n",
    "y_hat_binary, y_hat_prob = model.fitting(norm_x)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc_viz(grad):\n",
    "    # visualization the gradient descent\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    axes = fig.add_subplot(111)\n",
    "    axes.plot(grad)\n",
    "    axes.set_xlabel(\"iteration\")\n",
    "    axes.set_ylabel(\"gradnorm\")\n",
    "    plt.title(\"Gradient Descent of Model Training\")\n",
    "    plt.show()\n",
    "\n",
    "grad_desc_viz(grad=grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    accuracy = np.sum(np.equal(y, y_hat))/len(y)\n",
    "    return accuracy\n",
    "\n",
    "print(f\"The accuracy of the Logistic regression is {accuracy(np.array(data_y.values.tolist()).flatten(), np.array(y_hat_binary))}, spending {end-start}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    norm_x, data_y, test_size = .3, random_state=42,\n",
    "    stratify = data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training process with training data\n",
    "start = time.time()\n",
    "# initiate the theta\n",
    "init_theta = np.zeros(len(df_data.columns))\n",
    "# find the values\n",
    "model_train = Modeling(theta = init_theta, gamma = 0.00001, max_iters=1000)\n",
    "thetas_train, grad_train = model_train.gradient_iteration(x_train, y_train)\n",
    "y_hat_train,_ = model_train.fitting(x_train)\n",
    "y_hat_test,_ = model_train.fitting(x_test)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_desc_viz(grad_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tvbenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3c26eedd07840027ff202a94d88c89e67a86d8b5dcd58f087e1d46a589dbbcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
