{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Math Project Notebook\n",
    "\n",
    "This is a notebook for supervised machine learning project in Neural Network Mathematics class. \n",
    "\n",
    "Group members: Luke, Akshay, Yile\n",
    "\n",
    "#### variable names explanation:\n",
    "| Var name | Feature name | Description|\n",
    "|---|---|---|\n",
    "|pos      | Num posts    | Number of total posts that the user has ever posted.|\n",
    "|flg      | Num following | Number of following|\n",
    "|flr      | Num followers | Number of followers|\n",
    "|bl | Biography length | Length (number of characters) of the user's biography|\n",
    "|pic | Picture availability | Value 0 if the user has no profile picture, or 1 if has|\n",
    "|lin | Link availability | Value 0 if the user has no external URL, or 1 if has|\n",
    "|cl | Average caption length | The average number of character of captions in media|\n",
    "|cz | Caption zero | Percentage (0.0 to 1.0) of captions that has almost zero (<=3) length|\n",
    "|ni | Non image percentage | Percentage (0.0 to 1.0) of non-image media. There are three types of media on an Instagram post, i.e. image, video, carousel|\n",
    "|erl | Engagement rate (Like) | Engagement rate (ER) is commonly defined as (num likes) divide by (num media) divide by (num followers)|\n",
    "|erc | Engagement rate (Comm.) | Similar to ER like, but it is for comments|\n",
    "|lt | Location tag percentage | Percentage (0.0 to 1.0) of posts tagged with location|\n",
    "|hc | Average hashtag count | Average number of hashtags used in a post|\n",
    "|pr | Promotional keywords | Average use of promotional keywords in hashtag, i.e. {regrann, contest, repost, giveaway, mention, share, give away, quiz}|\n",
    "|fo | Followers keywords | Average use of followers hunter keywords in hashtag, i.e. {follow, like, folback, follback, f4f}|\n",
    "|cs | Cosine similarity | Average cosine similarity of between all pair of two posts a user has|\n",
    "|pi | Post interval | Average interval between posts (in hours)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic probability model is\n",
    "\n",
    "$ \\hat{p}(s, \\theta) = [1 + e^{-\\hat{y}(s, \\theta)}]^{-1} $\n",
    "\n",
    "The $\\hat{y}$ is defined as:\n",
    "\n",
    "$ \\hat{y}(s, \\theta) = \\theta^T [s^T 1]^T  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function is defined as\n",
    "\n",
    "$ c([y,s], \\theta) = - y  log\\hat{p}(s, \\theta) - (1-y)log(1-\\hat{p}(s, \\theta)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is\n",
    "\n",
    "$ l_{n}(\\theta) = -(1/n)\\sum_{i=1}^{n} c([y,s], \\theta) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent process is\n",
    "\n",
    "$ \\frac{dc_{i}}{d\\theta} = -(y_i - \\hat{y}_i) [s_i^{T}, 1] $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65321</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65322</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65323</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65324</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65325</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65326 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class\n",
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "...      ...\n",
       "65321      1\n",
       "65322      1\n",
       "65323      1\n",
       "65324      1\n",
       "65325      1\n",
       "\n",
       "[65326 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "df_data = pd.read_csv(\"data/user_fake_authentic_2class.csv\")\n",
    "# training features size: 65326 x 17\n",
    "data_x = df_data.iloc[:,:-1]\n",
    "\n",
    "# label types: r=real and f=fake\n",
    "data_y = df_data.iloc[:,-1:]\n",
    "# convert to 0:fake, 1:real\n",
    "data_y = data_y.replace({'class':{\"r\": 1, \"f\":0}})\n",
    "\n",
    "# for test\n",
    "#data_y = pd.DataFrame(data_y.iloc[:300,:])\n",
    "\n",
    "data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>flw</th>\n",
       "      <th>flg</th>\n",
       "      <th>bl</th>\n",
       "      <th>pic</th>\n",
       "      <th>lin</th>\n",
       "      <th>cl</th>\n",
       "      <th>cz</th>\n",
       "      <th>ni</th>\n",
       "      <th>erl</th>\n",
       "      <th>erc</th>\n",
       "      <th>lt</th>\n",
       "      <th>hc</th>\n",
       "      <th>pr</th>\n",
       "      <th>fo</th>\n",
       "      <th>cs</th>\n",
       "      <th>pi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.132007</td>\n",
       "      <td>0.144008</td>\n",
       "      <td>0.975053</td>\n",
       "      <td>0.099005</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020912</td>\n",
       "      <td>0.138019</td>\n",
       "      <td>0.671273</td>\n",
       "      <td>0.313679</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.030092</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.481838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029645</td>\n",
       "      <td>0.871381</td>\n",
       "      <td>0.276686</td>\n",
       "      <td>0.090731</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.391672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.009073</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.039140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.185676</td>\n",
       "      <td>0.228116</td>\n",
       "      <td>0.954904</td>\n",
       "      <td>0.037135</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.015543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009690</td>\n",
       "      <td>0.067827</td>\n",
       "      <td>0.920511</td>\n",
       "      <td>0.235780</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65321</th>\n",
       "      <td>0.006969</td>\n",
       "      <td>0.077732</td>\n",
       "      <td>0.344165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.935621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65322</th>\n",
       "      <td>0.194071</td>\n",
       "      <td>0.892962</td>\n",
       "      <td>0.386950</td>\n",
       "      <td>0.043458</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.114299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.016261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65323</th>\n",
       "      <td>0.292853</td>\n",
       "      <td>0.722370</td>\n",
       "      <td>0.624752</td>\n",
       "      <td>0.028700</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.025185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.025342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65324</th>\n",
       "      <td>0.137409</td>\n",
       "      <td>0.626483</td>\n",
       "      <td>0.751780</td>\n",
       "      <td>0.091049</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.121120</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.022304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65325</th>\n",
       "      <td>0.186527</td>\n",
       "      <td>0.596705</td>\n",
       "      <td>0.550526</td>\n",
       "      <td>0.024448</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.547793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65326 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pos       flw       flg        bl       pic       lin        cl  \\\n",
       "0      0.132007  0.144008  0.975053  0.099005  0.003000  0.000000  0.036002   \n",
       "1      0.020912  0.138019  0.671273  0.313679  0.002091  0.000000  0.445424   \n",
       "2      0.029645  0.871381  0.276686  0.090731  0.000898  0.000898  0.391672   \n",
       "3      0.185676  0.228116  0.954904  0.037135  0.002653  0.000000  0.000000   \n",
       "4      0.009690  0.067827  0.920511  0.235780  0.003230  0.000000  0.300377   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "65321  0.006969  0.077732  0.344165  0.000000  0.000536  0.000000  0.003753   \n",
       "65322  0.194071  0.892962  0.386950  0.043458  0.000298  0.000298  0.114299   \n",
       "65323  0.292853  0.722370  0.624752  0.028700  0.000195  0.000195  0.025185   \n",
       "65324  0.137409  0.626483  0.751780  0.091049  0.000418  0.000418  0.121120   \n",
       "65325  0.186527  0.596705  0.550526  0.024448  0.000905  0.000000  0.069721   \n",
       "\n",
       "             cz        ni       erl       erc        lt        hc   pr  \\\n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "1      0.000000  0.002091  0.030092  0.004120  0.000000  0.003137  0.0   \n",
       "2      0.000000  0.000898  0.009073  0.000269  0.000000  0.002246  0.0   \n",
       "3      0.002653  0.000000  0.002069  0.000159  0.000000  0.000000  0.0   \n",
       "4      0.000000  0.000000  0.046155  0.000000  0.002154  0.000000  0.0   \n",
       "...         ...       ...       ...       ...       ...       ...  ...   \n",
       "65321  0.000247  0.000000  0.007650  0.000311  0.000000  0.000041  0.0   \n",
       "65322  0.000000  0.000116  0.002536  0.000039  0.000000  0.000480  0.0   \n",
       "65323  0.000000  0.000022  0.001833  0.000061  0.000141  0.000000  0.0   \n",
       "65324  0.000023  0.000000  0.002652  0.000109  0.000093  0.000209  0.0   \n",
       "65325  0.000000  0.000302  0.023135  0.000480  0.000201  0.000201  0.0   \n",
       "\n",
       "             fo        cs        pi  \n",
       "0      0.000000  0.000333  0.000285  \n",
       "1      0.000000  0.000433  0.481838  \n",
       "2      0.000050  0.000514  0.039140  \n",
       "3      0.000000  0.002653  0.015543  \n",
       "4      0.000000  0.000971  0.000407  \n",
       "...         ...       ...       ...  \n",
       "65321  0.000000  0.000103  0.935621  \n",
       "65322  0.000000  0.000051  0.016261  \n",
       "65323  0.000011  0.000012  0.025342  \n",
       "65324  0.000000  0.000043  0.022304  \n",
       "65325  0.000151  0.000016  0.547793  \n",
       "\n",
       "[65326 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize \n",
    "norm_x = preprocessing.normalize(data_x)\n",
    "\n",
    "norm_x = pd.DataFrame(norm_x, columns=data_x.columns)\n",
    "\n",
    "norm_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd? Could implement a PCA for dimension reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class for logistic regression\n",
    "\n",
    "\"\"\"\n",
    "In our logistic regression model, there are several parameters need to be pre-defined:\n",
    "    1. gamma, learning rate\n",
    "    2. max_iters, the iteration number for the gradient descent\n",
    "    3. data_x, the training dataset\n",
    "    4. data_y, the prediction outcome\n",
    "\"\"\"\n",
    "\n",
    "class Modeling:\n",
    "\n",
    "    def __init__(self, theta, gamma = 0.0001, max_iters = 1000):\n",
    "        self.gamma = gamma\n",
    "        self.max_iters = max_iters\n",
    "        self.theta = theta\n",
    "        self.grad = None\n",
    "    \n",
    "\n",
    "    def _S_one(self, data_x):\n",
    "        data_x_yhat = data_x\n",
    "        data_x_yhat[\"y_hat\"] = np.ones(len(data_x.index))\n",
    "        return data_x_yhat\n",
    "    \n",
    "    def _y_hat(self, Si):\n",
    "        return self._logistic(np.dot(Si, self.theta))\n",
    "            \n",
    "    def _cost_function(self, Si, yi):\n",
    "        y_hat = self._y_hat(Si)\n",
    "        ci = -(np.log(y_hat)*yi + (1-yi)* np.log(1-y_hat))\n",
    "        return ci\n",
    "        \n",
    "    def _loss_func(self, S, data_y):\n",
    "        tmp = []\n",
    "        for row in range(len(S.index)):\n",
    "            ci = self._cost_function(S.iloc[row,:], data_y.iloc[row,:])\n",
    "            tmp.append(ci)\n",
    "        total = sum(tmp)\n",
    "        return 1/len(data_y)*total\n",
    "\n",
    "    \n",
    "    def _gradient_descent_func(self, Si, yi):\n",
    "        y_hat_i = self._y_hat(Si)\n",
    "        return -np.dot((yi - y_hat_i), Si)\n",
    "    \n",
    "    def gradient_iteration(self, S, data_y):\n",
    "        tmp = []\n",
    "        for row in range(len(S.index)):\n",
    "            d_loss = self._gradient_descent_func(S.iloc[row,:], data_y.values.flatten()[row])\n",
    "            tmp.append(d_loss)\n",
    "        total = sum(tmp)\n",
    "        return 1/len(data_y)*total\n",
    "\n",
    "        \n",
    "\n",
    "    # the objective function, y_hat = the prediction results from logistic regression\n",
    "    def gradient_descent(self, S, data_y):\n",
    "        \n",
    "        # gradient descent\n",
    "        grad = []\n",
    "        loss = []\n",
    "        t = 0\n",
    "        gradnorm = np.inf\n",
    "        while gradnorm >= 0.001 and t <= self.max_iters:\n",
    "            stime = time.time()\n",
    "            gt = self.gradient_iteration(S, data_y)\n",
    "            self.theta = self.theta - self.gamma*gt\n",
    "            gradnorm = max(abs(gt))\n",
    "            lss = self._loss_func(S, data_y)\n",
    "            loss.append(lss)\n",
    "            t += 1\n",
    "            etime = time.time()\n",
    "            print(f\"it takes {etime - stime} for each loop\")\n",
    "            grad.append(gradnorm)\n",
    "            print(f\"the iteration {t}, grad is {gradnorm}, loss is {lss}\")\n",
    "        return grad, loss\n",
    "        \n",
    "    def _logistic(self, x):\n",
    "        return (1/(1+np.exp(-x)))\n",
    "\n",
    "    def training(self, data_x, data_y):\n",
    "        S = self._S_one(data_x)\n",
    "        grad, loss = self.gradient_descent(S, data_y)\n",
    "        return grad, loss\n",
    "\n",
    "\n",
    "    def fitting(self, data_x):\n",
    "        S = self._S_one(data_x)\n",
    "        y_hat_list = []\n",
    "        for Si in range(len(S.index)):\n",
    "            y_hat_prob = self._y_hat(S.iloc[Si,:])\n",
    "            y_hat_list.append(y_hat_prob)\n",
    "        return y_hat_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model training process\n",
    "# start = time.time()\n",
    "# # initiate the theta\n",
    "# init_theta = np.zeros(len(df_data.columns))\n",
    "# # find the values\n",
    "# model = Modeling(theta = init_theta, gamma = 0.01, max_iters=500)\n",
    "# grad, loss = model.training(norm_x, data_y)\n",
    "# y_hat_list = model.fitting(norm_x)\n",
    "# y_hat_binary = [1 if i>0.5 else 0 for i in y_hat_list]\n",
    "# end = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc_viz(grad):\n",
    "    # visualization the gradient descent\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    axes = fig.add_subplot(111)\n",
    "    axes.plot(grad)\n",
    "    axes.set_xlabel(\"iteration\")\n",
    "    axes.set_ylabel(\"gradnorm\")\n",
    "    plt.title(\"Gradient Descent of Model Training\")\n",
    "    plt.show()\n",
    "\n",
    "# grad_desc_viz(grad=grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    accuracy = np.sum(np.equal(y, y_hat))/len(y)\n",
    "    return accuracy\n",
    "\n",
    "# print(f\"The accuracy of the Logistic regression is {accuracy(np.array(data_y.values.tolist()).flatten(), np.array(y_hat_binary))}, spending {end-start}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split\n",
    "x_train, x_leftover, y_train, y_leftover = train_test_split(\n",
    "    norm_x, data_y, test_size = .95, random_state=42,\n",
    "    stratify = data_y)\n",
    "\n",
    "\n",
    "x_test, x_leftover2, y_test, y_leftover2 = train_test_split(\n",
    "    x_leftover, y_leftover, test_size = .95, random_state=42,\n",
    "    stratify = y_leftover)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it takes 2.0162012577056885 for each loop\n",
      "the iteration 1, grad is 0.07149750731710512, loss is class    0.693038\n",
      "dtype: float64\n",
      "it takes 2.0314249992370605 for each loop\n",
      "the iteration 2, grad is 0.07142823320026015, loss is class    0.69293\n",
      "dtype: float64\n",
      "it takes 1.952010154724121 for each loop\n",
      "the iteration 3, grad is 0.07135916175523581, loss is class    0.692821\n",
      "dtype: float64\n",
      "it takes 1.999323844909668 for each loop\n",
      "the iteration 4, grad is 0.07129029217411044, loss is class    0.692713\n",
      "dtype: float64\n",
      "it takes 2.0031588077545166 for each loop\n",
      "the iteration 5, grad is 0.07122162365220808, loss is class    0.692604\n",
      "dtype: float64\n",
      "it takes 2.0706675052642822 for each loop\n",
      "the iteration 6, grad is 0.07115315538808582, loss is class    0.692496\n",
      "dtype: float64\n",
      "it takes 1.992095708847046 for each loop\n",
      "the iteration 7, grad is 0.07108488658352288, loss is class    0.692388\n",
      "dtype: float64\n",
      "it takes 1.9708971977233887 for each loop\n",
      "the iteration 8, grad is 0.07101681644350802, loss is class    0.69228\n",
      "dtype: float64\n",
      "it takes 2.0236730575561523 for each loop\n",
      "the iteration 9, grad is 0.07094894417622746, loss is class    0.692172\n",
      "dtype: float64\n",
      "it takes 1.955695629119873 for each loop\n",
      "the iteration 10, grad is 0.07088126899305411, loss is class    0.692064\n",
      "dtype: float64\n",
      "it takes 1.9691693782806396 for each loop\n",
      "the iteration 11, grad is 0.07081379010853424, loss is class    0.691956\n",
      "dtype: float64\n",
      "it takes 1.9325084686279297 for each loop\n",
      "the iteration 12, grad is 0.07074650674037708, loss is class    0.691849\n",
      "dtype: float64\n",
      "it takes 2.029505968093872 for each loop\n",
      "the iteration 13, grad is 0.07067941810944278, loss is class    0.691741\n",
      "dtype: float64\n",
      "it takes 1.95534348487854 for each loop\n",
      "the iteration 14, grad is 0.07061252343973065, loss is class    0.691634\n",
      "dtype: float64\n",
      "it takes 2.0073728561401367 for each loop\n",
      "the iteration 15, grad is 0.07054582195836681, loss is class    0.691526\n",
      "dtype: float64\n",
      "it takes 1.9737043380737305 for each loop\n",
      "the iteration 16, grad is 0.0704793128955935, loss is class    0.691419\n",
      "dtype: float64\n",
      "it takes 2.0078771114349365 for each loop\n",
      "the iteration 17, grad is 0.07041299548475685, loss is class    0.691312\n",
      "dtype: float64\n",
      "it takes 1.9294278621673584 for each loop\n",
      "the iteration 18, grad is 0.07034686896229639, loss is class    0.691205\n",
      "dtype: float64\n",
      "it takes 2.0421535968780518 for each loop\n",
      "the iteration 19, grad is 0.07028093256773182, loss is class    0.691098\n",
      "dtype: float64\n",
      "it takes 2.0125954151153564 for each loop\n",
      "the iteration 20, grad is 0.070215185543653, loss is class    0.690991\n",
      "dtype: float64\n",
      "it takes 1.9398694038391113 for each loop\n",
      "the iteration 21, grad is 0.07014962713570769, loss is class    0.690885\n",
      "dtype: float64\n",
      "it takes 2.012028694152832 for each loop\n",
      "the iteration 22, grad is 0.07008425659259039, loss is class    0.690778\n",
      "dtype: float64\n",
      "it takes 1.9697484970092773 for each loop\n",
      "the iteration 23, grad is 0.0700190731660307, loss is class    0.690672\n",
      "dtype: float64\n",
      "it takes 1.9970839023590088 for each loop\n",
      "the iteration 24, grad is 0.06995407611078239, loss is class    0.690565\n",
      "dtype: float64\n",
      "it takes 1.948103904724121 for each loop\n",
      "the iteration 25, grad is 0.0698892646846114, loss is class    0.690459\n",
      "dtype: float64\n",
      "it takes 1.9919793605804443 for each loop\n",
      "the iteration 26, grad is 0.06982463814828561, loss is class    0.690353\n",
      "dtype: float64\n",
      "it takes 1.939300537109375 for each loop\n",
      "the iteration 27, grad is 0.06976019576556224, loss is class    0.690247\n",
      "dtype: float64\n",
      "it takes 2.0150034427642822 for each loop\n",
      "the iteration 28, grad is 0.06969593680317783, loss is class    0.690141\n",
      "dtype: float64\n",
      "it takes 1.9605178833007812 for each loop\n",
      "the iteration 29, grad is 0.06963186053083534, loss is class    0.690035\n",
      "dtype: float64\n",
      "it takes 2.0297367572784424 for each loop\n",
      "the iteration 30, grad is 0.0695679662211958, loss is class    0.689929\n",
      "dtype: float64\n",
      "it takes 1.954195261001587 for each loop\n",
      "the iteration 31, grad is 0.069504253149864, loss is class    0.689823\n",
      "dtype: float64\n",
      "it takes 2.0154850482940674 for each loop\n",
      "the iteration 32, grad is 0.06944072059537917, loss is class    0.689718\n",
      "dtype: float64\n",
      "it takes 1.9531469345092773 for each loop\n",
      "the iteration 33, grad is 0.06937736783920459, loss is class    0.689612\n",
      "dtype: float64\n",
      "it takes 1.9974231719970703 for each loop\n",
      "the iteration 34, grad is 0.06931419416571379, loss is class    0.689507\n",
      "dtype: float64\n",
      "it takes 2.0129172801971436 for each loop\n",
      "the iteration 35, grad is 0.06925119886218233, loss is class    0.689402\n",
      "dtype: float64\n",
      "it takes 2.177004814147949 for each loop\n",
      "the iteration 36, grad is 0.0691883812187759, loss is class    0.689297\n",
      "dtype: float64\n",
      "it takes 2.2184786796569824 for each loop\n",
      "the iteration 37, grad is 0.06912574052853866, loss is class    0.689191\n",
      "dtype: float64\n",
      "it takes 2.0696160793304443 for each loop\n",
      "the iteration 38, grad is 0.069063276087383, loss is class    0.689086\n",
      "dtype: float64\n",
      "it takes 2.0222411155700684 for each loop\n",
      "the iteration 39, grad is 0.06900098719407888, loss is class    0.688982\n",
      "dtype: float64\n",
      "it takes 1.9623527526855469 for each loop\n",
      "the iteration 40, grad is 0.06893887315024236, loss is class    0.688877\n",
      "dtype: float64\n",
      "it takes 2.10256290435791 for each loop\n",
      "the iteration 41, grad is 0.06887693326032487, loss is class    0.688772\n",
      "dtype: float64\n",
      "it takes 2.0292978286743164 for each loop\n",
      "the iteration 42, grad is 0.06881516683160317, loss is class    0.688668\n",
      "dtype: float64\n",
      "it takes 2.22172474861145 for each loop\n",
      "the iteration 43, grad is 0.06875357317416729, loss is class    0.688563\n",
      "dtype: float64\n",
      "it takes 2.270289659500122 for each loop\n",
      "the iteration 44, grad is 0.06869215160091138, loss is class    0.688459\n",
      "dtype: float64\n",
      "it takes 2.378011465072632 for each loop\n",
      "the iteration 45, grad is 0.06863090142752183, loss is class    0.688354\n",
      "dtype: float64\n",
      "it takes 2.3260552883148193 for each loop\n",
      "the iteration 46, grad is 0.06856982197246667, loss is class    0.68825\n",
      "dtype: float64\n",
      "it takes 2.1788880825042725 for each loop\n",
      "the iteration 47, grad is 0.068508912556986, loss is class    0.688146\n",
      "dtype: float64\n",
      "it takes 1.9777252674102783 for each loop\n",
      "the iteration 48, grad is 0.06844817250507987, loss is class    0.688042\n",
      "dtype: float64\n",
      "it takes 2.1511802673339844 for each loop\n",
      "the iteration 49, grad is 0.06838760114349873, loss is class    0.687938\n",
      "dtype: float64\n",
      "it takes 2.0214333534240723 for each loop\n",
      "the iteration 50, grad is 0.06832719780173307, loss is class    0.687834\n",
      "dtype: float64\n",
      "it takes 1.9691693782806396 for each loop\n",
      "the iteration 51, grad is 0.06826696181200244, loss is class    0.687731\n",
      "dtype: float64\n",
      "it takes 2.0989506244659424 for each loop\n",
      "the iteration 52, grad is 0.0682068925092448, loss is class    0.687627\n",
      "dtype: float64\n",
      "it takes 1.9900188446044922 for each loop\n",
      "the iteration 53, grad is 0.06814698923110649, loss is class    0.687524\n",
      "dtype: float64\n",
      "it takes 2.081058979034424 for each loop\n",
      "the iteration 54, grad is 0.06808725131793192, loss is class    0.68742\n",
      "dtype: float64\n",
      "it takes 2.0091958045959473 for each loop\n",
      "the iteration 55, grad is 0.06802767811275298, loss is class    0.687317\n",
      "dtype: float64\n",
      "it takes 2.033651113510132 for each loop\n",
      "the iteration 56, grad is 0.06796826896127835, loss is class    0.687213\n",
      "dtype: float64\n",
      "it takes 2.2481882572174072 for each loop\n",
      "the iteration 57, grad is 0.06790902321188441, loss is class    0.68711\n",
      "dtype: float64\n",
      "it takes 2.2605247497558594 for each loop\n",
      "the iteration 58, grad is 0.06784994021560348, loss is class    0.687007\n",
      "dtype: float64\n",
      "it takes 2.2189245223999023 for each loop\n",
      "the iteration 59, grad is 0.06779101932611467, loss is class    0.686904\n",
      "dtype: float64\n",
      "it takes 2.331704616546631 for each loop\n",
      "the iteration 60, grad is 0.06773225989973283, loss is class    0.686801\n",
      "dtype: float64\n",
      "it takes 2.31003475189209 for each loop\n",
      "the iteration 61, grad is 0.0676736612953995, loss is class    0.686699\n",
      "dtype: float64\n",
      "it takes 2.0260727405548096 for each loop\n",
      "the iteration 62, grad is 0.06761522287467185, loss is class    0.686596\n",
      "dtype: float64\n",
      "it takes 2.2710986137390137 for each loop\n",
      "the iteration 63, grad is 0.06755694400171218, loss is class    0.686493\n",
      "dtype: float64\n",
      "it takes 2.3490307331085205 for each loop\n",
      "the iteration 64, grad is 0.06749882404328005, loss is class    0.686391\n",
      "dtype: float64\n",
      "it takes 2.349489688873291 for each loop\n",
      "the iteration 65, grad is 0.0674408623687189, loss is class    0.686289\n",
      "dtype: float64\n",
      "it takes 2.0719268321990967 for each loop\n",
      "the iteration 66, grad is 0.06738305834994934, loss is class    0.686186\n",
      "dtype: float64\n",
      "it takes 2.0428361892700195 for each loop\n",
      "the iteration 67, grad is 0.06732541136145691, loss is class    0.686084\n",
      "dtype: float64\n",
      "it takes 2.0151853561401367 for each loop\n",
      "the iteration 68, grad is 0.0672679207802832, loss is class    0.685982\n",
      "dtype: float64\n",
      "it takes 2.0500619411468506 for each loop\n",
      "the iteration 69, grad is 0.06721058598601569, loss is class    0.68588\n",
      "dtype: float64\n",
      "it takes 1.9585597515106201 for each loop\n",
      "the iteration 70, grad is 0.0671534063607778, loss is class    0.685778\n",
      "dtype: float64\n",
      "it takes 2.0316810607910156 for each loop\n",
      "the iteration 71, grad is 0.0670963812892188, loss is class    0.685676\n",
      "dtype: float64\n",
      "it takes 1.9545459747314453 for each loop\n",
      "the iteration 72, grad is 0.06703951015850494, loss is class    0.685574\n",
      "dtype: float64\n",
      "it takes 1.991133451461792 for each loop\n",
      "the iteration 73, grad is 0.06698279235830824, loss is class    0.685473\n",
      "dtype: float64\n",
      "it takes 1.9351356029510498 for each loop\n",
      "the iteration 74, grad is 0.06692622728079839, loss is class    0.685371\n",
      "dtype: float64\n",
      "it takes 2.0474159717559814 for each loop\n",
      "the iteration 75, grad is 0.06686981432063177, loss is class    0.685269\n",
      "dtype: float64\n",
      "it takes 1.9366035461425781 for each loop\n",
      "the iteration 76, grad is 0.06681355287494216, loss is class    0.685168\n",
      "dtype: float64\n",
      "it takes 2.014943838119507 for each loop\n",
      "the iteration 77, grad is 0.06675744234333052, loss is class    0.685067\n",
      "dtype: float64\n",
      "it takes 1.909503698348999 for each loop\n",
      "the iteration 78, grad is 0.06670148212785731, loss is class    0.684966\n",
      "dtype: float64\n",
      "it takes 1.9880173206329346 for each loop\n",
      "the iteration 79, grad is 0.06664567163303065, loss is class    0.684864\n",
      "dtype: float64\n",
      "it takes 1.9946446418762207 for each loop\n",
      "the iteration 80, grad is 0.06659001026579744, loss is class    0.684763\n",
      "dtype: float64\n",
      "it takes 2.023984909057617 for each loop\n",
      "the iteration 81, grad is 0.06653449743553459, loss is class    0.684662\n",
      "dtype: float64\n",
      "it takes 2.3438899517059326 for each loop\n",
      "the iteration 82, grad is 0.0664791325540392, loss is class    0.684562\n",
      "dtype: float64\n",
      "it takes 2.006277084350586 for each loop\n",
      "the iteration 83, grad is 0.06642391503551845, loss is class    0.684461\n",
      "dtype: float64\n",
      "it takes 2.025582790374756 for each loop\n",
      "the iteration 84, grad is 0.06636884429658013, loss is class    0.68436\n",
      "dtype: float64\n",
      "it takes 1.9343538284301758 for each loop\n",
      "the iteration 85, grad is 0.06631391975622526, loss is class    0.684259\n",
      "dtype: float64\n",
      "it takes 2.0322306156158447 for each loop\n",
      "the iteration 86, grad is 0.06625914083583574, loss is class    0.684159\n",
      "dtype: float64\n",
      "it takes 1.9504494667053223 for each loop\n",
      "the iteration 87, grad is 0.06620450695916692, loss is class    0.684059\n",
      "dtype: float64\n",
      "it takes 1.9879498481750488 for each loop\n",
      "the iteration 88, grad is 0.06615001755233821, loss is class    0.683958\n",
      "dtype: float64\n",
      "it takes 1.934326410293579 for each loop\n",
      "the iteration 89, grad is 0.0660956720438234, loss is class    0.683858\n",
      "dtype: float64\n",
      "it takes 2.028696298599243 for each loop\n",
      "the iteration 90, grad is 0.06604146986444097, loss is class    0.683758\n",
      "dtype: float64\n",
      "it takes 1.930304765701294 for each loop\n",
      "the iteration 91, grad is 0.06598741044734573, loss is class    0.683658\n",
      "dtype: float64\n",
      "it takes 2.045727252960205 for each loop\n",
      "the iteration 92, grad is 0.06593349322801921, loss is class    0.683558\n",
      "dtype: float64\n",
      "it takes 1.9405426979064941 for each loop\n",
      "the iteration 93, grad is 0.06587971764426108, loss is class    0.683458\n",
      "dtype: float64\n",
      "it takes 1.9927117824554443 for each loop\n",
      "the iteration 94, grad is 0.06582608313617902, loss is class    0.683358\n",
      "dtype: float64\n",
      "it takes 2.0112102031707764 for each loop\n",
      "the iteration 95, grad is 0.06577258914618017, loss is class    0.683258\n",
      "dtype: float64\n",
      "it takes 1.9376311302185059 for each loop\n",
      "the iteration 96, grad is 0.06571923511896234, loss is class    0.683159\n",
      "dtype: float64\n",
      "it takes 1.9937858581542969 for each loop\n",
      "the iteration 97, grad is 0.06566602050150484, loss is class    0.683059\n",
      "dtype: float64\n",
      "it takes 1.9238803386688232 for each loop\n",
      "the iteration 98, grad is 0.06561294474305902, loss is class    0.68296\n",
      "dtype: float64\n",
      "it takes 2.0254597663879395 for each loop\n",
      "the iteration 99, grad is 0.06556000729514012, loss is class    0.68286\n",
      "dtype: float64\n",
      "it takes 1.906470537185669 for each loop\n",
      "the iteration 100, grad is 0.06550720761151793, loss is class    0.682761\n",
      "dtype: float64\n",
      "it takes 1.9640476703643799 for each loop\n",
      "the iteration 101, grad is 0.06545454514820681, loss is class    0.682662\n",
      "dtype: float64\n",
      "The accuracy of the Logistic regression is 0.713410900183711, spending 206.73666286468506s\n",
      "The accuracy of the Logistic regression is 0.7009345794392523, spending 206.73666286468506s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model training process with training data\n",
    "start = time.time()\n",
    "# initiate the theta\n",
    "init_theta = np.zeros(len(df_data.columns))\n",
    "# find the values\n",
    "model_train = Modeling(theta = init_theta, gamma = 0.01, max_iters=100)\n",
    "thetas_train, grad_train = model_train.training(x_train, y_train)\n",
    "y_hat_train_list = model_train.fitting(x_train)\n",
    "y_hat_test_list = model_train.fitting(x_test)\n",
    "y_hat_train_binary = [1 if i>0.5 else 0 for i in y_hat_train_list]\n",
    "y_hat_test_binary = [1 if i>0.5 else 0 for i in y_hat_test_list]\n",
    "\n",
    "end = time.time()\n",
    "print(f\"The accuracy of the Logistic regression is {accuracy(np.array(y_train.values.tolist()).flatten(), np.array(y_hat_train_binary))}, spending {end-start}s\")\n",
    "\n",
    "print(f\"The accuracy of the Logistic regression is {accuracy(np.array(y_test.values.tolist()).flatten(), np.array(y_hat_test_binary))}, spending {end-start}s\")\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "grad_desc_viz(grad_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tvbenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d6eff59d82162ad618c2fda16bbe4a2b1e156e75fbd6961cfe85de3ca5351f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
