{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Math Project Notebook\n",
    "\n",
    "This is a notebook for supervised machine learning project in Neural Network Mathematics class. \n",
    "\n",
    "Group members: Luke, Akshay, Yile\n",
    "\n",
    "#### variable names explanation:\n",
    "| Var name | Feature name | Description|\n",
    "|---|---|---|\n",
    "|pos      | Num posts    | Number of total posts that the user has ever posted.|\n",
    "|flg      | Num following | Number of following|\n",
    "|flr      | Num followers | Number of followers|\n",
    "|bl | Biography length | Length (number of characters) of the user's biography|\n",
    "|pic | Picture availability | Value 0 if the user has no profile picture, or 1 if has|\n",
    "|lin | Link availability | Value 0 if the user has no external URL, or 1 if has|\n",
    "|cl | Average caption length | The average number of character of captions in media|\n",
    "|cz | Caption zero | Percentage (0.0 to 1.0) of captions that has almost zero (<=3) length|\n",
    "|ni | Non image percentage | Percentage (0.0 to 1.0) of non-image media. There are three types of media on an Instagram post, i.e. image, video, carousel|\n",
    "|erl | Engagement rate (Like) | Engagement rate (ER) is commonly defined as (num likes) divide by (num media) divide by (num followers)|\n",
    "|erc | Engagement rate (Comm.) | Similar to ER like, but it is for comments|\n",
    "|lt | Location tag percentage | Percentage (0.0 to 1.0) of posts tagged with location|\n",
    "|hc | Average hashtag count | Average number of hashtags used in a post|\n",
    "|pr | Promotional keywords | Average use of promotional keywords in hashtag, i.e. {regrann, contest, repost, giveaway, mention, share, give away, quiz}|\n",
    "|fo | Followers keywords | Average use of followers hunter keywords in hashtag, i.e. {follow, like, folback, follback, f4f}|\n",
    "|cs | Cosine similarity | Average cosine similarity of between all pair of two posts a user has|\n",
    "|pi | Post interval | Average interval between posts (in hours)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic probability model is\n",
    "\n",
    "$ \\hat{p}(s, \\theta) = [1 + e^{-\\hat{y}(s, \\theta)}]^{-1} $\n",
    "\n",
    "The $\\hat{y}$ is defined as:\n",
    "\n",
    "$ \\hat{y}(s, \\theta) = \\theta^T [s^T 1]^T  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function is defined as\n",
    "\n",
    "$ c([y,s], \\theta) = - y  log\\hat{p}(s, \\theta) - (1-y)log(1-\\hat{p}(s, \\theta)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is\n",
    "\n",
    "$ l_{n}(\\theta) = -(1/n)\\sum_{i=1}^{n} c([y,s], \\theta) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent process is\n",
    "\n",
    "$ \\frac{dc_{i}}{d\\theta} = -(y_i - \\hat{y}_i) [s_i^{T}, 1] $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>flw</th>\n",
       "      <th>flg</th>\n",
       "      <th>bl</th>\n",
       "      <th>pic</th>\n",
       "      <th>lin</th>\n",
       "      <th>cl</th>\n",
       "      <th>cz</th>\n",
       "      <th>ni</th>\n",
       "      <th>erl</th>\n",
       "      <th>erc</th>\n",
       "      <th>lt</th>\n",
       "      <th>hc</th>\n",
       "      <th>pr</th>\n",
       "      <th>fo</th>\n",
       "      <th>cs</th>\n",
       "      <th>pi</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>48</td>\n",
       "      <td>325</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.094985</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>66</td>\n",
       "      <td>321</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>14.390000</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.206826</td>\n",
       "      <td>230.412857</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>970</td>\n",
       "      <td>308</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.572174</td>\n",
       "      <td>43.569939</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>360</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.859799</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>285</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.290000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300494</td>\n",
       "      <td>0.126019</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65321</th>\n",
       "      <td>13</td>\n",
       "      <td>145</td>\n",
       "      <td>642</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.270000</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>1745.291260</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65322</th>\n",
       "      <td>652</td>\n",
       "      <td>3000</td>\n",
       "      <td>1300</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389</td>\n",
       "      <td>8.520000</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.169917</td>\n",
       "      <td>54.629120</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65323</th>\n",
       "      <td>1500</td>\n",
       "      <td>3700</td>\n",
       "      <td>3200</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111</td>\n",
       "      <td>9.390000</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.058908</td>\n",
       "      <td>129.802048</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65324</th>\n",
       "      <td>329</td>\n",
       "      <td>1500</td>\n",
       "      <td>1800</td>\n",
       "      <td>218</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>290</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.103174</td>\n",
       "      <td>53.402840</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65325</th>\n",
       "      <td>206</td>\n",
       "      <td>659</td>\n",
       "      <td>608</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333</td>\n",
       "      <td>25.549999</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.017505</td>\n",
       "      <td>604.981445</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65326 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pos   flw   flg   bl  pic  lin   cl        cz     ni        erl   erc  \\\n",
       "0        44    48   325   33    1    0   12  0.000000  0.000   0.000000  0.00   \n",
       "1        10    66   321  150    1    0  213  0.000000  1.000  14.390000  1.97   \n",
       "2        33   970   308  101    1    1  436  0.000000  1.000  10.100000  0.30   \n",
       "3        70    86   360   14    1    0    0  1.000000  0.000   0.780000  0.06   \n",
       "4         3    21   285   73    1    0   93  0.000000  0.000  14.290000  0.00   \n",
       "...     ...   ...   ...  ...  ...  ...  ...       ...    ...        ...   ...   \n",
       "65321    13   145   642    0    1    0    7  0.461538  0.000  14.270000  0.58   \n",
       "65322   652  3000  1300  146    1    1  384  0.000000  0.389   8.520000  0.13   \n",
       "65323  1500  3700  3200  147    1    1  129  0.000000  0.111   9.390000  0.31   \n",
       "65324   329  1500  1800  218    1    1  290  0.055556  0.000   6.350000  0.26   \n",
       "65325   206   659   608   27    1    0   77  0.000000  0.333  25.549999  0.53   \n",
       "\n",
       "          lt     hc   pr     fo        cs           pi class  \n",
       "0      0.000  0.000  0.0  0.000  0.111111     0.094985     f  \n",
       "1      0.000  1.500  0.0  0.000  0.206826   230.412857     f  \n",
       "2      0.000  2.500  0.0  0.056  0.572174    43.569939     f  \n",
       "3      0.000  0.000  0.0  0.000  1.000000     5.859799     f  \n",
       "4      0.667  0.000  0.0  0.000  0.300494     0.126019     f  \n",
       "...      ...    ...  ...    ...       ...          ...   ...  \n",
       "65321  0.000  0.077  0.0  0.000  0.192308  1745.291260     r  \n",
       "65322  0.000  1.611  0.0  0.000  0.169917    54.629120     r  \n",
       "65323  0.722  0.000  0.0  0.056  0.058908   129.802048     r  \n",
       "65324  0.222  0.500  0.0  0.000  0.103174    53.402840     r  \n",
       "65325  0.222  0.222  0.0  0.167  0.017505   604.981445     r  \n",
       "\n",
       "[65326 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "df_data = pd.read_csv(\"data/user_fake_authentic_2class.csv\")\n",
    "# training features size: 65326 x 17\n",
    "data_x = df_data.iloc[:,:-1]\n",
    "\n",
    "# label types: r=real and f=fake\n",
    "data_y = df_data.iloc[:,-1:]\n",
    "# convert to 0:fake, 1:real\n",
    "data_y = data_y.replace({'class':{\"r\": 1, \"f\":0}})\n",
    "\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>flw</th>\n",
       "      <th>flg</th>\n",
       "      <th>bl</th>\n",
       "      <th>pic</th>\n",
       "      <th>lin</th>\n",
       "      <th>cl</th>\n",
       "      <th>cz</th>\n",
       "      <th>ni</th>\n",
       "      <th>erl</th>\n",
       "      <th>erc</th>\n",
       "      <th>lt</th>\n",
       "      <th>hc</th>\n",
       "      <th>pr</th>\n",
       "      <th>fo</th>\n",
       "      <th>cs</th>\n",
       "      <th>pi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.132007</td>\n",
       "      <td>0.144008</td>\n",
       "      <td>0.975053</td>\n",
       "      <td>0.099005</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020912</td>\n",
       "      <td>0.138019</td>\n",
       "      <td>0.671273</td>\n",
       "      <td>0.313679</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.030092</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.481838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029645</td>\n",
       "      <td>0.871381</td>\n",
       "      <td>0.276686</td>\n",
       "      <td>0.090731</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.391672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.009073</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.039140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.185676</td>\n",
       "      <td>0.228116</td>\n",
       "      <td>0.954904</td>\n",
       "      <td>0.037135</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.015543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009690</td>\n",
       "      <td>0.067827</td>\n",
       "      <td>0.920511</td>\n",
       "      <td>0.235780</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65321</th>\n",
       "      <td>0.006969</td>\n",
       "      <td>0.077732</td>\n",
       "      <td>0.344165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.935621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65322</th>\n",
       "      <td>0.194071</td>\n",
       "      <td>0.892962</td>\n",
       "      <td>0.386950</td>\n",
       "      <td>0.043458</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.114299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.016261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65323</th>\n",
       "      <td>0.292853</td>\n",
       "      <td>0.722370</td>\n",
       "      <td>0.624752</td>\n",
       "      <td>0.028700</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.025185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.025342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65324</th>\n",
       "      <td>0.137409</td>\n",
       "      <td>0.626483</td>\n",
       "      <td>0.751780</td>\n",
       "      <td>0.091049</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.121120</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.022304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65325</th>\n",
       "      <td>0.186527</td>\n",
       "      <td>0.596705</td>\n",
       "      <td>0.550526</td>\n",
       "      <td>0.024448</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.547793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65326 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pos       flw       flg        bl       pic       lin        cl  \\\n",
       "0      0.132007  0.144008  0.975053  0.099005  0.003000  0.000000  0.036002   \n",
       "1      0.020912  0.138019  0.671273  0.313679  0.002091  0.000000  0.445424   \n",
       "2      0.029645  0.871381  0.276686  0.090731  0.000898  0.000898  0.391672   \n",
       "3      0.185676  0.228116  0.954904  0.037135  0.002653  0.000000  0.000000   \n",
       "4      0.009690  0.067827  0.920511  0.235780  0.003230  0.000000  0.300377   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "65321  0.006969  0.077732  0.344165  0.000000  0.000536  0.000000  0.003753   \n",
       "65322  0.194071  0.892962  0.386950  0.043458  0.000298  0.000298  0.114299   \n",
       "65323  0.292853  0.722370  0.624752  0.028700  0.000195  0.000195  0.025185   \n",
       "65324  0.137409  0.626483  0.751780  0.091049  0.000418  0.000418  0.121120   \n",
       "65325  0.186527  0.596705  0.550526  0.024448  0.000905  0.000000  0.069721   \n",
       "\n",
       "             cz        ni       erl       erc        lt        hc   pr  \\\n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "1      0.000000  0.002091  0.030092  0.004120  0.000000  0.003137  0.0   \n",
       "2      0.000000  0.000898  0.009073  0.000269  0.000000  0.002246  0.0   \n",
       "3      0.002653  0.000000  0.002069  0.000159  0.000000  0.000000  0.0   \n",
       "4      0.000000  0.000000  0.046155  0.000000  0.002154  0.000000  0.0   \n",
       "...         ...       ...       ...       ...       ...       ...  ...   \n",
       "65321  0.000247  0.000000  0.007650  0.000311  0.000000  0.000041  0.0   \n",
       "65322  0.000000  0.000116  0.002536  0.000039  0.000000  0.000480  0.0   \n",
       "65323  0.000000  0.000022  0.001833  0.000061  0.000141  0.000000  0.0   \n",
       "65324  0.000023  0.000000  0.002652  0.000109  0.000093  0.000209  0.0   \n",
       "65325  0.000000  0.000302  0.023135  0.000480  0.000201  0.000201  0.0   \n",
       "\n",
       "             fo        cs        pi  \n",
       "0      0.000000  0.000333  0.000285  \n",
       "1      0.000000  0.000433  0.481838  \n",
       "2      0.000050  0.000514  0.039140  \n",
       "3      0.000000  0.002653  0.015543  \n",
       "4      0.000000  0.000971  0.000407  \n",
       "...         ...       ...       ...  \n",
       "65321  0.000000  0.000103  0.935621  \n",
       "65322  0.000000  0.000051  0.016261  \n",
       "65323  0.000011  0.000012  0.025342  \n",
       "65324  0.000000  0.000043  0.022304  \n",
       "65325  0.000151  0.000016  0.547793  \n",
       "\n",
       "[65326 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize \n",
    "norm_x = preprocessing.normalize(data_x)\n",
    "norm_x = pd.DataFrame(norm_x, columns=data_x.columns)\n",
    "\n",
    "norm_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd? Could implement a PCA for dimension reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class for logistic regression\n",
    "\n",
    "\"\"\"\n",
    "In our logistic regression model, there are several parameters need to be pre-defined:\n",
    "    1. gamma, learning rate\n",
    "    2. max_iters, the iteration number for the gradient descent\n",
    "    3. data_x, the training dataset\n",
    "    4. data_y, the prediction outcome\n",
    "\"\"\"\n",
    "\n",
    "class Modeling:\n",
    "\n",
    "    def __init__(self, theta, gamma = 0.0001, max_iters = 1000):\n",
    "        self.gamma = gamma\n",
    "        self.max_iters = max_iters\n",
    "        self.theta = theta\n",
    "        self.grad = None\n",
    "    \n",
    "\n",
    "    def _s_one(self, data_x):\n",
    "        data_x_yhat = data_x\n",
    "        data_x_yhat[\"y_hat\"] = np.ones(len(data_x.index))\n",
    "        return data_x_yhat\n",
    "\n",
    "    def _y_hat_func(self, data_x_yhat):\n",
    "        y_hat = np.matmul(data_x_yhat, self.theta)\n",
    "        return y_hat\n",
    "            \n",
    "    def _loss_func(self, x):\n",
    "        objective_func = -1 * data_y * np.log(self._logistic(x)) - (1-data_y) * np.log(1-self._logistic(x))\n",
    "        loss = 1/len(data_y)* sum(objective_func)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def _gradient_descent_func(self, data_y, y_hat, data_x_yhat):\n",
    "        gradient = np.matmul(-1*(np.array(data_y).flatten() - y_hat).T, np.array(data_x_yhat))\n",
    "        gradient_vector = gradient.flatten()\n",
    "        return gradient_vector\n",
    "\n",
    "    # the objective function, y_hat = the prediction results from logistic regression\n",
    "    def gradient_iteration(self, data_x, data_y):\n",
    "        # gradient descent\n",
    "        grad = []\n",
    "        t = 0\n",
    "        gradnorm = np.inf\n",
    "        while gradnorm >= 0.001 and t <= self.max_iters:\n",
    "            data_x_yhat  = self._s_one(data_x)\n",
    "            y_hat = self._y_hat_func(data_x_yhat)\n",
    "            # The gradient descent \n",
    "            gradient_loss = self._gradient_descent_func(data_y, y_hat, data_x_yhat)\n",
    "            gt = gradient_loss\n",
    "            self.theta -= self.gamma*gt\n",
    "            gradnorm = max(abs(gt))\n",
    "            t += 1\n",
    "            # print(f\"Iternation: {t}; gradnorm = {gradnorm}\")\n",
    "            grad.append(gradnorm)\n",
    "        return self.theta, grad\n",
    "        \n",
    "    def _logistic(self, x):\n",
    "        return (1/(1+np.exp(-x)))\n",
    "\n",
    "    def fitting(self, data_x):\n",
    "        data_x_y_hat = self._s_one(data_x)\n",
    "        y_hat_prob = self._logistic(self._y_hat_func(data_x_y_hat))\n",
    "        y_hat_binary = [1 if i>0.5 else 0 for i in y_hat_prob]\n",
    "        return y_hat_binary, y_hat_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training process\n",
    "start = time.time()\n",
    "# initiate the theta\n",
    "init_theta = np.zeros(len(df_data.columns))\n",
    "# find the values\n",
    "model = Modeling(theta = init_theta, gamma = 0.0001, max_iters=1000)\n",
    "thetas, grad = model.gradient_iteration(norm_x, data_y)\n",
    "y_hat_binary, y_hat_prob = model.fitting(norm_x)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def grad_desc_viz(grad):\n",
    "    # visualization the gradient descent\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    axes = fig.add_subplot(111)\n",
    "    axes.plot(grad)\n",
    "    axes.set_xlabel(\"iteration\")\n",
    "    axes.set_ylabel(\"gradnorm\")\n",
    "    plt.title(\"Gradient Descent of Model Training\")\n",
    "    plt.show()\n",
    "\n",
    "grad_desc_viz(grad=grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Logistic regression is 0.5031074916572268, spending 2.005422353744507s\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    accuracy = np.sum(np.equal(y, y_hat))/len(y)\n",
    "    return accuracy\n",
    "\n",
    "print(f\"The accuracy of the Logistic regression is {accuracy(np.array(data_y.values.tolist()).flatten(), np.array(y_hat_binary))}, spending {end-start}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    norm_x, data_y, test_size = .3, random_state=42,\n",
    "    stratify = data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Modeling' object has no attribute 'objective_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# find the values\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model_train \u001b[39m=\u001b[39m Modeling(theta \u001b[39m=\u001b[39m init_theta, gamma \u001b[39m=\u001b[39m \u001b[39m0.00001\u001b[39m, max_iters\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m thetas_train, grad_train \u001b[39m=\u001b[39m model_train\u001b[39m.\u001b[39;49mobjective_func(x_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m y_hat_train,_ \u001b[39m=\u001b[39m model_train\u001b[39m.\u001b[39mfitting(x_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/yat-lok/workspace/NeuralNetMath_Project/logistic_regression_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m y_hat_test,_ \u001b[39m=\u001b[39m model_train\u001b[39m.\u001b[39mfitting(x_test, y_test)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Modeling' object has no attribute 'objective_func'"
     ]
    }
   ],
   "source": [
    "# model training process with training data\n",
    "start = time.time()\n",
    "# initiate the theta\n",
    "init_theta = np.zeros(len(df_data.columns))\n",
    "# find the values\n",
    "model_train = Modeling(theta = init_theta, gamma = 0.00001, max_iters=1000)\n",
    "thetas_train, grad_train = model_train.objective_func(x_train, y_train)\n",
    "y_hat_train,_ = model_train.fitting(x_train)\n",
    "y_hat_test,_ = model_train.fitting(x_test)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_desc_viz(grad_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tvbenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3c26eedd07840027ff202a94d88c89e67a86d8b5dcd58f087e1d46a589dbbcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
