{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Math Project Notebook\n",
    "\n",
    "This is a notebook for supervised machine learning project in Neural Network Mathematics class. \n",
    "\n",
    "Group members: Luke, Akshay, Yile\n",
    "\n",
    "#### variable names explanation:\n",
    "| Var name | Feature name | Description|\n",
    "|---|---|---|\n",
    "|pos      | Num posts    | Number of total posts that the user has ever posted.|\n",
    "|flg      | Num following | Number of following|\n",
    "|flr      | Num followers | Number of followers|\n",
    "|bl | Biography length | Length (number of characters) of the user's biography|\n",
    "|pic | Picture availability | Value 0 if the user has no profile picture, or 1 if has|\n",
    "|lin | Link availability | Value 0 if the user has no external URL, or 1 if has|\n",
    "|cl | Average caption length | The average number of character of captions in media|\n",
    "|cz | Caption zero | Percentage (0.0 to 1.0) of captions that has almost zero (<=3) length|\n",
    "|ni | Non image percentage | Percentage (0.0 to 1.0) of non-image media. There are three types of media on an Instagram post, i.e. image, video, carousel|\n",
    "|erl | Engagement rate (Like) | Engagement rate (ER) is commonly defined as (num likes) divide by (num media) divide by (num followers)|\n",
    "|erc | Engagement rate (Comm.) | Similar to ER like, but it is for comments|\n",
    "|lt | Location tag percentage | Percentage (0.0 to 1.0) of posts tagged with location|\n",
    "|hc | Average hashtag count | Average number of hashtags used in a post|\n",
    "|pr | Promotional keywords | Average use of promotional keywords in hashtag, i.e. {regrann, contest, repost, giveaway, mention, share, give away, quiz}|\n",
    "|fo | Followers keywords | Average use of followers hunter keywords in hashtag, i.e. {follow, like, folback, follback, f4f}|\n",
    "|cs | Cosine similarity | Average cosine similarity of between all pair of two posts a user has|\n",
    "|pi | Post interval | Average interval between posts (in hours)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic probability model is\n",
    "\n",
    "$ \\hat{p}(s, \\theta) = [1 + e^{-\\hat{y}(s, \\theta)}]^{-1} $\n",
    "\n",
    "The $\\hat{y}$ is defined as:\n",
    "\n",
    "$ \\hat{y}(s, \\theta) = \\theta^T [s^T 1]^T  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function is defined as\n",
    "\n",
    "$ c([y,s], \\theta) = - y  log\\hat{p}(s, \\theta) - (1-y)log(1-\\hat{p}(s, \\theta)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is\n",
    "\n",
    "$ l_{n}(\\theta) = -(1/n)\\sum_{i=1}^{n} c([y,s], \\theta) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent process is\n",
    "\n",
    "$ \\frac{dc_{i}}{d\\theta} = -(y_i - \\hat{y}_i) [s_i^{T}, 1] $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65321</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65322</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65323</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65324</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65325</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65326 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class\n",
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "...      ...\n",
       "65321      1\n",
       "65322      1\n",
       "65323      1\n",
       "65324      1\n",
       "65325      1\n",
       "\n",
       "[65326 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "df_data = pd.read_csv(\"data/user_fake_authentic_2class.csv\")\n",
    "# training features size: 65326 x 17\n",
    "data_x = df_data.iloc[:,:-1]\n",
    "\n",
    "# label types: r=real and f=fake\n",
    "data_y = df_data.iloc[:,-1:]\n",
    "# convert to 0:fake, 1:real\n",
    "data_y = data_y.replace({'class':{\"r\": 1, \"f\":0}})\n",
    "\n",
    "# for test\n",
    "#data_y = pd.DataFrame(data_y.iloc[:300,:])\n",
    "\n",
    "data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>flw</th>\n",
       "      <th>flg</th>\n",
       "      <th>bl</th>\n",
       "      <th>pic</th>\n",
       "      <th>lin</th>\n",
       "      <th>cl</th>\n",
       "      <th>cz</th>\n",
       "      <th>ni</th>\n",
       "      <th>erl</th>\n",
       "      <th>erc</th>\n",
       "      <th>lt</th>\n",
       "      <th>hc</th>\n",
       "      <th>pr</th>\n",
       "      <th>fo</th>\n",
       "      <th>cs</th>\n",
       "      <th>pi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.132007</td>\n",
       "      <td>0.144008</td>\n",
       "      <td>0.975053</td>\n",
       "      <td>0.099005</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020912</td>\n",
       "      <td>0.138019</td>\n",
       "      <td>0.671273</td>\n",
       "      <td>0.313679</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.030092</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.481838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029645</td>\n",
       "      <td>0.871381</td>\n",
       "      <td>0.276686</td>\n",
       "      <td>0.090731</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.391672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.009073</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.039140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.185676</td>\n",
       "      <td>0.228116</td>\n",
       "      <td>0.954904</td>\n",
       "      <td>0.037135</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.015543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009690</td>\n",
       "      <td>0.067827</td>\n",
       "      <td>0.920511</td>\n",
       "      <td>0.235780</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65321</th>\n",
       "      <td>0.006969</td>\n",
       "      <td>0.077732</td>\n",
       "      <td>0.344165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.935621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65322</th>\n",
       "      <td>0.194071</td>\n",
       "      <td>0.892962</td>\n",
       "      <td>0.386950</td>\n",
       "      <td>0.043458</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.114299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.016261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65323</th>\n",
       "      <td>0.292853</td>\n",
       "      <td>0.722370</td>\n",
       "      <td>0.624752</td>\n",
       "      <td>0.028700</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.025185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.025342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65324</th>\n",
       "      <td>0.137409</td>\n",
       "      <td>0.626483</td>\n",
       "      <td>0.751780</td>\n",
       "      <td>0.091049</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.121120</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.022304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65325</th>\n",
       "      <td>0.186527</td>\n",
       "      <td>0.596705</td>\n",
       "      <td>0.550526</td>\n",
       "      <td>0.024448</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.547793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65326 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pos       flw       flg        bl       pic       lin        cl  \\\n",
       "0      0.132007  0.144008  0.975053  0.099005  0.003000  0.000000  0.036002   \n",
       "1      0.020912  0.138019  0.671273  0.313679  0.002091  0.000000  0.445424   \n",
       "2      0.029645  0.871381  0.276686  0.090731  0.000898  0.000898  0.391672   \n",
       "3      0.185676  0.228116  0.954904  0.037135  0.002653  0.000000  0.000000   \n",
       "4      0.009690  0.067827  0.920511  0.235780  0.003230  0.000000  0.300377   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "65321  0.006969  0.077732  0.344165  0.000000  0.000536  0.000000  0.003753   \n",
       "65322  0.194071  0.892962  0.386950  0.043458  0.000298  0.000298  0.114299   \n",
       "65323  0.292853  0.722370  0.624752  0.028700  0.000195  0.000195  0.025185   \n",
       "65324  0.137409  0.626483  0.751780  0.091049  0.000418  0.000418  0.121120   \n",
       "65325  0.186527  0.596705  0.550526  0.024448  0.000905  0.000000  0.069721   \n",
       "\n",
       "             cz        ni       erl       erc        lt        hc   pr  \\\n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "1      0.000000  0.002091  0.030092  0.004120  0.000000  0.003137  0.0   \n",
       "2      0.000000  0.000898  0.009073  0.000269  0.000000  0.002246  0.0   \n",
       "3      0.002653  0.000000  0.002069  0.000159  0.000000  0.000000  0.0   \n",
       "4      0.000000  0.000000  0.046155  0.000000  0.002154  0.000000  0.0   \n",
       "...         ...       ...       ...       ...       ...       ...  ...   \n",
       "65321  0.000247  0.000000  0.007650  0.000311  0.000000  0.000041  0.0   \n",
       "65322  0.000000  0.000116  0.002536  0.000039  0.000000  0.000480  0.0   \n",
       "65323  0.000000  0.000022  0.001833  0.000061  0.000141  0.000000  0.0   \n",
       "65324  0.000023  0.000000  0.002652  0.000109  0.000093  0.000209  0.0   \n",
       "65325  0.000000  0.000302  0.023135  0.000480  0.000201  0.000201  0.0   \n",
       "\n",
       "             fo        cs        pi  \n",
       "0      0.000000  0.000333  0.000285  \n",
       "1      0.000000  0.000433  0.481838  \n",
       "2      0.000050  0.000514  0.039140  \n",
       "3      0.000000  0.002653  0.015543  \n",
       "4      0.000000  0.000971  0.000407  \n",
       "...         ...       ...       ...  \n",
       "65321  0.000000  0.000103  0.935621  \n",
       "65322  0.000000  0.000051  0.016261  \n",
       "65323  0.000011  0.000012  0.025342  \n",
       "65324  0.000000  0.000043  0.022304  \n",
       "65325  0.000151  0.000016  0.547793  \n",
       "\n",
       "[65326 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize \n",
    "norm_x = preprocessing.normalize(data_x)\n",
    "\n",
    "norm_x = pd.DataFrame(norm_x, columns=data_x.columns)\n",
    "\n",
    "norm_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd? Could implement a PCA for dimension reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class for logistic regression\n",
    "\n",
    "\"\"\"\n",
    "In our logistic regression model, there are several parameters need to be pre-defined:\n",
    "    1. gamma, learning rate\n",
    "    2. max_iters, the iteration number for the gradient descent\n",
    "    3. data_x, the training dataset\n",
    "    4. data_y, the prediction outcome\n",
    "\"\"\"\n",
    "\n",
    "class Modeling:\n",
    "\n",
    "    def __init__(self, theta, gamma = 0.0001, max_iters = 1000):\n",
    "        self.gamma = gamma\n",
    "        self.max_iters = max_iters\n",
    "        self.theta = theta\n",
    "        self.grad = None\n",
    "    \n",
    "\n",
    "    def _S_one(self, data_x):\n",
    "        data_x_yhat = data_x\n",
    "        data_x_yhat[\"y_hat\"] = np.ones(len(data_x.index))\n",
    "        return data_x_yhat\n",
    "    \n",
    "    def _y_hat(self, Si):\n",
    "        return self._logistic(np.dot(Si, self.theta))\n",
    "            \n",
    "    def _cost_function(self, Si, yi):\n",
    "        y_hat = self._y_hat(Si)\n",
    "        ci = -(np.log(y_hat)*yi + (1-yi)* np.log(1-y_hat))\n",
    "        return ci\n",
    "        \n",
    "    def _loss_func(self, S, data_y):\n",
    "        tmp = []\n",
    "        for row in range(len(S.index)):\n",
    "            ci = self._cost_function(S.iloc[row,:], data_y.iloc[row,:])\n",
    "            tmp.append(ci)\n",
    "        total = sum(tmp)\n",
    "        return 1/len(data_y)*total\n",
    "\n",
    "    \n",
    "    def _gradient_descent_func(self, Si, yi):\n",
    "        y_hat_i = self._y_hat(Si)\n",
    "        return -np.dot((yi - y_hat_i), Si)\n",
    "    \n",
    "    def gradient_iteration(self, S, data_y):\n",
    "        tmp = []\n",
    "        for row in range(len(S.index)):\n",
    "            d_loss = self._gradient_descent_func(S.iloc[row,:], data_y.values.flatten()[row])\n",
    "            tmp.append(d_loss)\n",
    "        total = sum(tmp)\n",
    "        return 1/len(data_y)*total\n",
    "\n",
    "        \n",
    "\n",
    "    # the objective function, y_hat = the prediction results from logistic regression\n",
    "    def gradient_descent(self, S, data_y):\n",
    "        \n",
    "        # gradient descent\n",
    "        grad = []\n",
    "        loss = []\n",
    "        t = 0\n",
    "        gradnorm = np.inf\n",
    "        while gradnorm >= 0.001 and t <= self.max_iters:\n",
    "            stime = time.time()\n",
    "            gt = self.gradient_iteration(S, data_y)\n",
    "            self.theta = self.theta - self.gamma*gt\n",
    "            gradnorm = max(abs(gt))\n",
    "            lss = self._loss_func(S, data_y)\n",
    "            loss.append(lss)\n",
    "            t += 1\n",
    "            etime = time.time()\n",
    "            #print(f\"it takes {etime - stime} for each loop\")\n",
    "            grad.append(gradnorm)\n",
    "            #print(f\"the iteration {t}, grad is {gradnorm}, loss is {lss}\")\n",
    "        return grad, loss\n",
    "        \n",
    "    def _logistic(self, x):\n",
    "        return (1/(1+np.exp(-x)))\n",
    "\n",
    "    def training(self, data_x, data_y):\n",
    "        S = self._S_one(data_x)\n",
    "        grad, loss = self.gradient_descent(S, data_y)\n",
    "        return grad, loss\n",
    "\n",
    "\n",
    "    def fitting(self, data_x):\n",
    "        S = self._S_one(data_x)\n",
    "        y_hat_list = []\n",
    "        for Si in range(len(S.index)):\n",
    "            y_hat_prob = self._y_hat(S.iloc[Si,:])\n",
    "            y_hat_list.append(y_hat_prob)\n",
    "        return y_hat_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model training process\n",
    "# start = time.time()\n",
    "# # initiate the theta\n",
    "# init_theta = np.zeros(len(df_data.columns))\n",
    "# # find the values\n",
    "# model = Modeling(theta = init_theta, gamma = 0.01, max_iters=500)\n",
    "# grad, loss = model.training(norm_x, data_y)\n",
    "# y_hat_list = model.fitting(norm_x)\n",
    "# y_hat_binary = [1 if i>0.5 else 0 for i in y_hat_list]\n",
    "# end = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc_viz(grad):\n",
    "    # visualization the gradient descent\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    axes = fig.add_subplot(111)\n",
    "    axes.plot(grad)\n",
    "    axes.set_xlabel(\"iteration\")\n",
    "    axes.set_ylabel(\"gradnorm\")\n",
    "    plt.title(\"Gradient Descent of Model Training\")\n",
    "    plt.show()\n",
    "\n",
    "# grad_desc_viz(grad=grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    accuracy = np.sum(np.equal(y, y_hat))/len(y)\n",
    "    return accuracy\n",
    "\n",
    "# print(f\"The accuracy of the Logistic regression is {accuracy(np.array(data_y.values.tolist()).flatten(), np.array(y_hat_binary))}, spending {end-start}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split\n",
    "x_train, x_leftover, y_train, y_leftover = train_test_split(\n",
    "    norm_x, data_y, test_size = .95, random_state=42,\n",
    "    stratify = data_y)\n",
    "\n",
    "\n",
    "x_test, x_leftover2, y_test, y_leftover2 = train_test_split(\n",
    "    x_leftover, y_leftover, test_size = .95, random_state=42,\n",
    "    stratify = y_leftover)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model training process with training data\n",
    "start = time.time()\n",
    "# initiate the theta\n",
    "init_theta = np.zeros(len(df_data.columns))\n",
    "# find the values\n",
    "model_train = Modeling(theta = init_theta, gamma = 0.01, max_iters=1000)\n",
    "thetas_train, grad_train = model_train.training(x_train, y_train)\n",
    "y_hat_train_list = model_train.fitting(x_train)\n",
    "y_hat_test_list = model_train.fitting(x_test)\n",
    "y_hat_train_binary = [1 if i>0.5 else 0 for i in y_hat_train_list]\n",
    "y_hat_test_binary = [1 if i>0.5 else 0 for i in y_hat_test_list]\n",
    "\n",
    "end = time.time()\n",
    "print(f\"The accuracy of the Logistic regression is {accuracy(np.array(y_train.values.tolist()).flatten(), np.array(y_hat_train_binary))}, spending {end-start}s\")\n",
    "\n",
    "print(f\"The accuracy of the Logistic regression is {accuracy(np.array(y_test.values.tolist()).flatten(), np.array(y_hat_test_binary))}, spending {end-start}s\")\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_desc_viz(grad_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tvbenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d6eff59d82162ad618c2fda16bbe4a2b1e156e75fbd6961cfe85de3ca5351f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
